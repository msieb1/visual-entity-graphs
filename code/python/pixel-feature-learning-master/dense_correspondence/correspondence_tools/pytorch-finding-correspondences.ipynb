{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of finding correspondences\n",
    "\n",
    "- Demos generating correspondences in PyTorch\n",
    "- Simple timing experiments in pytorch\n",
    "- Demos generating non-correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find pixel correspondences in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "utils.add_dense_correspondence_to_python_path()\n",
    "import correspondence_plotter\n",
    "from dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'dense_correspondence', \n",
    "                               'dataset', 'composite', 'caterpillar_only.yaml')\n",
    "config = utils.getDictFromYamlFilename(config_filename)\n",
    "dataset = SpartanDataset(config=config)\n",
    "\n",
    "import correspondence_finder\n",
    "import time\n",
    "uv_a = (300,200)\n",
    "\n",
    "scene = \"2018-04-10-16-02-59\"\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a = (300,200)\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    uv_a=uv_a)\n",
    "print time.time() - start, \"seconds\"\n",
    "if uv_a is not None:\n",
    "    correspondence_plotter.plot_correspondences_direct(img_a_rgb, img_a_depth_numpy, img_b_rgb, img_b_depth_numpy, uv_a, uv_b)\n",
    "else:\n",
    "    print \"try running this cell again, did not find a correspondence for this pixel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation -- finding many correspondences\n",
    "\n",
    "Note that in this example, about 1/10 get pruned due to either:\n",
    "\n",
    "1. No depth measurement in image a\n",
    "2. Reprojection is outside FOV of image b\n",
    "3. Occluded: the point from image a is occluded in image b\n",
    "4. No depth measurement in image b (so can't be sure if occluded or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "num_attempts = 50\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose, \n",
    "                                                                    num_attempts=num_attempts)\n",
    "print time.time() - start, \"seconds\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])\n",
    "if uv_a is not None:\n",
    "    correspondence_plotter.plot_correspondences_direct(img_a_rgb, img_a_depth_numpy, img_b_rgb, img_b_depth_numpy, uv_a, uv_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CPU, how fast can we do many, many samples? (many samples, let's not plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attempts = 50000\n",
    "\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='CPU')\n",
    "\n",
    "print time.time() - start, \"seconds 1st time\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='CPU')\n",
    "\n",
    "print time.time() - start, \"seconds 2nd time\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])\n",
    "\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='CPU')\n",
    "print time.time() - start, \"seconds on a new image pair\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GPU, how fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attempts = 50000\n",
    "\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='GPU')\n",
    "\n",
    "print time.time() - start, \"seconds 1st time\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='GPU')\n",
    "\n",
    "print time.time() - start, \"seconds 2nd time\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])\n",
    "\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='GPU')\n",
    "print time.time() - start, \"seconds on a new image pair\"\n",
    "print \"num attempted: \", num_attempts\n",
    "print \"num valid:     \", len(uv_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Comments\n",
    "\n",
    "- Since PyTorch has an asynchronous cuda API, the timing numbers for the GPU version are not really valid\n",
    "- In the end, we actually prefer to perform correspondence matching on CPU, since we can have the PyTorch multithreaded dataset loader be performing correspondence matching in parallel with no cost to the GPU, and then the GPU is reserved for just forwards-backwards passes of the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attempts = 5\n",
    "\n",
    "img_a_index = dataset.get_random_image_index(scene)\n",
    "img_a_rgb, img_a_depth, _, img_a_pose = dataset.get_rgbd_mask_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = dataset.get_img_idx_with_different_pose(scene, img_a_pose, num_attempts=50)\n",
    "img_b_rgb, img_b_depth, _, img_b_pose = dataset.get_rgbd_mask_pose(scene, img_b_index)\n",
    "\n",
    "img_a_depth_numpy = np.asarray(img_a_depth)\n",
    "img_b_depth_numpy = np.asarray(img_b_depth)\n",
    "\n",
    "start = time.time()\n",
    "uv_a, uv_b = correspondence_finder.batch_find_pixel_correspondences(img_a_depth_numpy, img_a_pose, \n",
    "                                                                    img_b_depth_numpy, img_b_pose,\n",
    "                                                                    num_attempts=num_attempts,\n",
    "                                                                    device='CPU')\n",
    "\n",
    "start = time.time()\n",
    "uv_b_non_matches = correspondence_finder.create_non_correspondences(uv_b, img_a_depth_numpy.shape, num_non_matches_per_match=10)\n",
    "print  time.time() - start, \"seconds for non-matches\"\n",
    "if uv_b_non_matches is not None:\n",
    "    print uv_b_non_matches[0].shape\n",
    "\n",
    "    import torch\n",
    "    # This just checks to make sure nothing is out of bounds\n",
    "    print torch.min(uv_b_non_matches[0])\n",
    "    print torch.min(uv_b_non_matches[1])\n",
    "    print torch.max(uv_b_non_matches[0])\n",
    "    print torch.max(uv_b_non_matches[1])\n",
    "    \n",
    "    fig, axes = correspondence_plotter.plot_correspondences_direct(img_a_rgb, img_a_depth_numpy, img_b_rgb, img_b_depth_numpy, uv_a, uv_b, show=False)\n",
    "    uv_a_long = (torch.t(uv_a[0].repeat(3, 1)).contiguous().view(-1,1), torch.t(uv_a[1].repeat(3, 1)).contiguous().view(-1,1))\n",
    "    uv_b_non_matches_long = (uv_b_non_matches[0].view(-1,1), uv_b_non_matches[1].view(-1,1) )\n",
    "    correspondence_plotter.plot_correspondences_direct(img_a_rgb, img_a_depth_numpy, img_b_rgb, img_b_depth_numpy, uv_a_long, uv_b_non_matches_long, use_previous_plot=(fig,axes),\n",
    "                                                  circ_color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note for future:\n",
    "\n",
    "In the plot above (at least at time of this writing), you can see a potential issue that would want a little bit of a refactor.\n",
    "\n",
    "It is currently possible for \"non-matches\" to sample parts of image b for which there is no known depth.\n",
    "\n",
    "This could be an issue for example, if it just so happens that that corner of image b matches image a.\n",
    "\n",
    "Two things though:\n",
    "\n",
    "1. Once I have depths from the actual projection against world model, this will be less of an issue, since there will be less holes.\n",
    "2. That combined with that issue hopefully being rare, means maybe I shouldn't worry about it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
